>   **SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: Group Number      |
|-----------------|
| Tanish Datta                |   
| Arushi Gupta              |   
| Rian Opperman               |   
| Seniru Ruwanpura                |   


**Table of Contents**

1. [Introduction](#intro)

2. [High-level description of the exploratory testing plan](#high)

3. [Comparison of exploratory and manual functional testing](#comp)

4. [Notes and discussion of the peer reviews of defect reports](#notes)

5. [How the pair testing was managed and team work/effort was divided](#how)

6. [Difficulties encountered, challenges overcome, and lessons learned](#diff)

7. [Comments/feedback on the lab and lab document itself](#comm)

# Introduction <a name="intro"></a>

Through this lab we were able to develop a preliminary understanding of the fundamentals of software testing, and in specific the three main kinds of testing: exploratory, manual (scripted), and regression testing. This was done by applying these testing methodologies in order on a desktop ATM machine simulation that was able to transfer, withdraw, and deposit money into various accounts through various credit cards, with the additional ability to choose how much money the ATM started with as well. Firstly, we started with exploratory testing, which as we understood was based around creating and executing test cases simultaneously by playing around with the program and conducting trials of the various functions that the program was able to exhibit. This preliminary process was also based around trying to understand how to write well-written test cases, for the second phase: manual testing. In essence, the exploratory testing confirmed our initial thoughts based on the class about the process: learn the program, create and execute tests all at the same time. Manual functional testing was something all of us had to discuss at length before committing to, as it was a new process. Based on what we had learnt prior, manual testing was centered around the idea of following already laid-out test cases, and executing those test cases to see the validity of the program’s functional requirements. While it was confusing how the breakup was done (having 2 pairs for exploratory and then manual was individual), it became far easier to understand when we looked at the already outlined test cases that were written, which meant that all we had to do was go through those cases and confirm or deny whether the program met those requirements. Furthermore, as the test cases were written to a high level of detail this process became even easier as we went through it. Finally, we went through regression testing, which we also were not fully aware about and thus had to do a lot of discussion before committing to the process. From what we did understand, regression testing was centered around the process of simply re-testing old software that had been altered (in the hopes of fixing the bugs) and making sure that all fixes applied properly and that new bugs were not inherent in the system. We largely discovered this to be the case by downloading a newer version of the ATM system and testing it. However, something new we recognized during the lab is that regression is not its own testing procedure per se. Rather, we learnt that we needed to repeat both the exploratory and the manual testing processes again to ensure that through both processes, the software had been fixed.

# High-level description of the exploratory testing plan <a name="high"></a>

Our exploratory testing was broken into two pairs as was asked.

The first pair (Arushi and Seniru) broke their exploratory testing into two main phases. First, Arushi created and executed some tests to see if there was anything that was obviously and fundamentally wrong with the program. This was done in a completely random fashion, testing as many aspects as possible with the program. When bugs were discovered, Seniru put them into the Jira testing software, and this process was continued for some period of time. After this, the process became a lot more methodical, with Seniru testing the withdrawal and balance inquiry features of the application, and Arushi testing the deposit and transfer functional requirements. The most important aspects of this was making sure that when each of these functions were triggered, that the balance remained accurate (for available and total) and additional money was not taken from the account nor was extra money added. To get to each of these stages for the two of them, firstly the credit card number and the pin numbers were checked, using a correct and wrong number to make sure wrong access was not granted and that when the right numbers were inputted the account was accessed. Regarding withdrawal, deposit, and transfer various figures were tested to make sure the right amount was added or reduced from the account, whereas with the balance inquiry the various options were just triggered. Testing out all of these allowed us to not only test the functionality of the overall user functional requirements in the application, but also things like button functionality as well.  

The second pair (Rian and Tanish) started off by creating a high-level plan for which sections of the program to test, like withdrawals and so forth, and from there specified common program errors to test; for example, withdrawing more money than what was in your account. We then followed this plan methodically to test all sections highlighted in the plan, and delved deeper on some tests, like withdrawing money, when encountering the bug where the withdrawal amount on the screen did not match the amount dispensed by the machine. Overall the idea was to follow the plan loosely and to experiment and dig into certain functionalities more when something went awry. This was mostly done in the spirit of how the user would use the program, and since the user always finds bugs the team never noticed, we decided that this would increase our chances of finding odd or tricky defects that would normally go unnoticed until the program hit production.

# Comparison of exploratory and manual functional testing <a name="comp"></a>

Between the Exploratory and Manual Functional Testing (MFT) done, generally we found more unusual bugs with exploratory testing, as we were free to dig deeper into testing when a functionality had an error. Exploratory testing was a lot slower and generally less effective than MFT as we tested on the fly, meaning we could get distracted more often on one functionality. However, due to the nature of Exploratory testing, it still found more unique bugs than MFT as randomly testing some functionality results in an unusual path taken through the application; leading to finding bugs that would not have been found with rigid, structured testing. MFT did have some key advantages over Exploratory testing though, mainly the efficiency and effectiveness of finding bugs, as the structured path through the program allowed testing to be done much quicker than Exploratory testing. In general though, our Exploratory testing phase found more bugs and even covered some of the use cases in the MFT phase, meaning MFT was only really useful in testing edge cases for the application.

# Notes and discussion of the peer reviews of defect reports <a name="notes"></a>

After we finished our pair testing defect reports, we sat down and compared the bugs that we found. Both teams had many similarities with the bugs that they found. Some of the main functionalities that we had both tested were the 3 main options, Withdrawal, Deposit, and Transfer. We noticed that we did a lot of the tests in different orders which allowed us to tackle the app through different aspects. By not having an order during the exploratory testing phase, it allowed for a different viewpoint of the system, consequently allowing us to find versatile bugs. This allowed for some bugs to be found that were unique to the group. Both teams also made it very easy to follow along the bugs that they found as Seniru and Arushi used Jira and Excel to record the defects that they found and Tanish and Rian used Jira and Markdown files. This made it extremely easy to put all defects into Jira and update it as we moved through the 3 stages of testing; exploratory, manual and regression testing. Also, due to the different test values that both pairs used, all numeric functions were tested through a range of values ensuring all input possibilities were tested. In general, having the two bug reports really allowed us to look through the application with a fine toothed comb and helped to bring out different aspects of the defects found within the application.

# How the pair testing was managed and team work/effort was divided <a name="how"></a>

Overall team work/effort was divided when we first met for the lab on January 20th and then further through communication on discord. Based on previous work experience/familiarity with others in the group’s work patterns, Arushi and Seniru formed a pair and then Rian and Tanish formed the second pair, for the exploratory phase.

Arushi and Seniru first had a system where one individual showed the application and tested for various bugs and the other put the bugs into Jira. Then, of the four major aspects of the ATM program, they divided this into two and then made themselves responsible for not only creating and executing the test, but also reporting it as well. Rian and Tanish replicated the former aspect of this process, where Rian created and executed tests and Tanish reported them in Jira. Once the exploratory phase was finished, the pairs merged back into the overall group and we split up the 40 Manual Functional Testing cases into 4 groups of 10 and assigned 10 cases for each individual. We tested the cases individual and then reported it into the Jira backlog for the 1.0 version of the ATM. Much of this process was then repeated for Regression testing, where the pairs got together again to repeat the exploratory test cases for ATM 1.1. Once this was done, we split up the cases to give each member 10 cases each to do the manual testing component of Regression testing. Finally, if a group member was able to complete tests quickly they did an extra amount of tests, while the group members who were left with less tests worked more on finishing up the report.

# Difficulties encountered, challenges overcome, and lessons learned <a name="diff"></a>

Generally the only difficulties encountered was trying to adapt to the new system of Jira as our bug reporting service, and documenting the bugs in detail, as generally in previous years it was not needed to report bugs at all. However, after learning about Jira more and about effective documentation of bugs we realized that creating these highly detailed reports gives the developers a much better idea of what to focus on in debugging, how they can reproduce the error and how they might fix it. This is obviously invaluable to teams working on big projects because the sheer scale of the application means a non-descriptive bug report would take a lot of time for the developer to decipher and find. Software like Jira also is quite useful for bigger teams as it fits perfectly in an Agile development methodology, allowing different team members to be assigned certain bugs to fix, and allows both the testing and development team to share the bugs encountered. Another lesson we learned was how to use GitHub and GitHub Classroom more effectively, including the use of Markdown files to write the report.

# Comments/feedback on the lab and lab document itself <a name="comm"></a>

We found the Lab itself to be quite useful on how testing is done in industry and helped strengthen our idea of the different testing methods. Using the software itself, like jira, was also very informative on how the industry requires these applications to not only document bugs, but distribute it around a team of developers. The lab document itself was well organized and instructions were quite clear, however there were some aspects of the application that would have been nice to include, like what “Total Balance” and “Available Balance” was instead of figuring it out through using the program. In all honesty this was one of the best assignments/labs we have gotten as of late, since it was descriptive and left nothing out. The only thing we wish we would have gotten was a bit more time, as trying to figure everything out in a week with other commitments made it a slight time-crunch.
